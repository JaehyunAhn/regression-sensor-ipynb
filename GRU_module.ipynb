{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import random\n",
    "from keras.layers import Dropout, LSTM, Dense, Activation, Embedding, GRU\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " 다른 디바이스에서 사용하기\n",
    "with tf.device('/gpu:0'):\n",
    "광역으로 with tf.device('/gpu:' + GPUNUM): 멕이고\n",
    "세션 선언시\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    GET DATA LIST ON FOLDER\n",
    "    * Input value   : folder\n",
    "    * Process       : open .csv files from folder, get sensor list and sort\n",
    "    * Return value  : Pandas DataFrame\n",
    "\"\"\"\n",
    "def get_data_list_on_folder(folder, complete_set=False):\n",
    "    folder_list = os.listdir(folder)\n",
    "    data_list = []\n",
    "\n",
    "    # get item list from folder\n",
    "    for csvs in folder_list:\n",
    "        with open(str(folder + '/' + csvs), 'r') as csvfile:\n",
    "            i = 0\n",
    "            sensor_data = csv.reader(csvfile, quotechar='\"')\n",
    "            for data in sensor_data:\n",
    "                data_list.append(data)\n",
    "                i += 1\n",
    "                if (i % 100000 == 0):\n",
    "                    sys.stdout.write('\\r%s, Loading %dK list.' % (csvs, i/1000))\n",
    "                    sys.stdout.flush()\n",
    "            print('\\n%s, %d line process done.' % (csvs, i))\n",
    "\n",
    "    # print & sort data / Sort by a time\n",
    "    print('Sorting...')\n",
    "    data_list = sorted(data_list, key=lambda data_list:data_list[1])\n",
    "    print('Set Dataframe...')\n",
    "    # set dataframe on pandas\n",
    "    Dust = []\n",
    "    Light = []\n",
    "    VoC = []\n",
    "    Co2 = []\n",
    "    Temp = []\n",
    "    Humid =[]\n",
    "    # uncertain value\n",
    "    E = []\n",
    "    Oxy = []\n",
    "    X = []\n",
    "    R = []\n",
    "    M = []\n",
    "    j = 0\n",
    "    for sensor in data_list:\n",
    "        kind = sensor[6] # D,L,V,C,T,H\n",
    "        value = sensor[4]\n",
    "        try:\n",
    "            value = float(sensor[4])\n",
    "        except ValueError:\n",
    "            j += 1\n",
    "            # if value is NULL skip that line\n",
    "            continue\n",
    "        if (kind == \"D\"):\n",
    "            Dust.append(value)\n",
    "        elif (kind == \"L\"):\n",
    "            Light.append(value)\n",
    "        elif (kind == \"V\"):\n",
    "            VoC.append(value)\n",
    "        elif (kind == \"C\"):\n",
    "            Co2.append(value)\n",
    "        elif (kind == \"T\"):\n",
    "            Temp.append(value)\n",
    "        elif (kind == \"H\"):\n",
    "            Humid.append(value)\n",
    "        elif (kind == \"E\"):\n",
    "            E.append(value)\n",
    "        elif (kind == \"O\"):\n",
    "            Oxy.append(value)\n",
    "        elif (kind == \"X\"):\n",
    "            X.append(value)\n",
    "        elif (kind == \"R\"):\n",
    "            R.append(value)\n",
    "        elif (kind == \"M\"):\n",
    "            M.append(value)\n",
    "        else:\n",
    "            print(\"[Warning] sensor error in %s. Please check commons.py 58 line.\" % str(sensor))\n",
    "    print(\"[Note] Result <D: %d, L: %d, V: %d, C: %d, T: %d, H: %d, E: %d, O: %d, X: %d, R: %d, M: %d>\" %\n",
    "          (len(Dust), len(Light), len(VoC), len(Co2), len(Temp), len(Humid), len(E), len(Oxy), len(X), len(R), len(M)))\n",
    "    print(\"[Note] %d number of value is NULL\" % (j))\n",
    "    # Setting Data\n",
    "    if (complete_set is False):\n",
    "        # 6 items: DLVCTH\n",
    "        min_line = min(len(Dust), len(Light), len(VoC), len(Co2), len(Temp), len(Humid))\n",
    "        min_line -= 1\n",
    "        data = pd.DataFrame({\"D\": Dust[:min_line], \"L\": Light[:min_line], \"V\": VoC[:min_line], \"C\": Co2[:min_line],\n",
    "                            \"T\": Temp[:min_line], \"H\": Humid[:min_line]})\n",
    "    elif (complete_set is True):\n",
    "        # 11 items: DLVCTH + EOXRM\n",
    "        min_line = min(len(Dust), len(Light), len(VoC), len(Co2), len(Temp), len(Humid), len(E), len(Oxy), len(X), len(R), len(M))\n",
    "        min_line -= 1\n",
    "        data = pd.DataFrame({\"D\": Dust[:min_line], \"L\": Light[:min_line], \"V\": VoC[:min_line], \"C\": Co2[:min_line],\n",
    "                            \"T\": Temp[:min_line], \"H\": Humid[:min_line], \"E\": E[:min_line], \"O\": Oxy[:min_line],\n",
    "                            \"X\": X[:min_line], \"R\": R[:min_line], \"M\": M[:min_line]})\n",
    "\n",
    "    print('[DONE] Returned %d items from %s' % (len(data_list), folder))\n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "    _load_data\n",
    "    * Note: Data should be pd.DataFrame()\n",
    "    * Input value:\n",
    "        - data = pd.DataFrame()\n",
    "        - n_prev = # of previous consider value (Default is 100) as a Learning Set (like corpus)\n",
    "    * Output value:\n",
    "        - training matrix(numpy array)\n",
    "\"\"\"\n",
    "def _load_data(data, n_prev = 100):\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev].as_matrix()) # data.iloc\n",
    "        docY.append(data.iloc[i+n_prev].as_matrix())\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "\n",
    "    return alsX, alsY\n",
    "\n",
    "\"\"\"\n",
    "    train_test_split\n",
    "\"\"\"\n",
    "def train_test_split(df, fold_size=0.1, time_steps=100):\n",
    "    print('[Note] Setting Train/Test set separation.')\n",
    "    ntrn = round(len(df) * (1 - fold_size))\n",
    "    ntrn = int(ntrn)\n",
    "    X_train, y_train = _load_data(df.iloc[0:ntrn], n_prev=time_steps)\n",
    "    X_test, y_test = _load_data(df.iloc[ntrn:], n_prev=time_steps) # df.iloc\n",
    "    print('X_train.shape: %s' % str(X_train.shape))\n",
    "    print('y_train.shape: %s' % str(y_train.shape))\n",
    "    print('[DONE] Train/Test set ready.')\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def data_separation():\n",
    "    print('Operating data separation module.')\n",
    "    with open('160920_sensor.csv', 'r') as csvfile:\n",
    "        sensor_data = csv.reader(csvfile, quotechar='\"')\n",
    "        i = 0\n",
    "        for row in sensor_data:\n",
    "            id = '160921_sensor'\n",
    "            # Check header & separator\n",
    "            # About 32,650,000 Lines\n",
    "            if(i==0):\n",
    "                header_list = row\n",
    "            else:\n",
    "                # id, time, user, serial, value, ip, type, user2\n",
    "                separator = str(row[5])\n",
    "                id = id + separator + '.csv'\n",
    "            i += 1\n",
    "\n",
    "            # open and write 'row'\n",
    "            writer = ''\n",
    "            try:\n",
    "                f = open(id, 'a')#, newline='')\n",
    "                writer = csv.DictWriter(f, fieldnames=header_list)\n",
    "            except IOError: # FileNotFoundError\n",
    "                print(separator + ' ip generated.')\n",
    "                f = open(id, 'w+')#, newline='')\n",
    "                writer = csv.DictWriter(f, fieldnames=header_list)\n",
    "                writer.writeheader()\n",
    "            writer.writerow({'id': row[0], 'time': row[1], 'user': row[2],\n",
    "                             'serial': row[3], 'value': row[4], 'ip': row[5],\n",
    "                             'type': row[6],'user2': row[7]})\n",
    "            f.close()\n",
    "            if(i%100000 == 0):\n",
    "                sys.stdout.write('%.2f%% is done.' % (float(i)/32650000)*100)\n",
    "                sys.stdout.flush()\n",
    "    return 1\n",
    "\n",
    "\"\"\"\n",
    "    Get close vector for estimation\n",
    "\"\"\"\n",
    "def get_close_vector(vector, real_data, model_data):\n",
    "    # set mapping to model, model to mapping vector\n",
    "    if(len(real_data) == len(model_data)):\n",
    "        print(\"[Note] Calculating...\")\n",
    "        vec_val = vector.sum()\n",
    "        vec_idx = -1\n",
    "        min_val = np.amax(model_data, axis=0).sum()\n",
    "        for i in range(len(model_data)):\n",
    "            temp_val = abs(model_data[i].sum() - vector.sum())\n",
    "            if(temp_val < min_val):\n",
    "                min_val = temp_val\n",
    "                vec_idx = i\n",
    "            else:\n",
    "                continue\n",
    "        if(vec_idx != -1):\n",
    "            print(\"[Note] D: %.2f, L: %.2f, V: %.2f, C: %.2f, T: %.2f, H: %.2f\" % \n",
    "                  (real_data[vec_idx][0], real_data[vec_idx][1], real_data[vec_idx][2], real_data[vec_idx][3],\n",
    "                  real_data[vec_idx][4], real_data[vec_idx][5]))\n",
    "            return real_data[vec_idx]\n",
    "    else:\n",
    "        print(\"[Error] Cannot creat mapping list. Because number of lists are not match. (%d, %d)\" %(len(real_data), len(model_data)))\n",
    "        return None\n",
    "    print(\"[Warning] Somethings got wrong. Please check <create_mapping_function>.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "160921_sensor1.225.5.24.csv, 5404 line process done.\n",
      "\n",
      "160921_sensor59.14.19.207.csv, 34 line process done.\n",
      "160921_sensor59.11.85.58.csv, Loading 21700K list.\n",
      "160921_sensor59.11.85.58.csv, 21776029 line process done.\n",
      "Sorting...\n",
      "Set Dataframe...\n",
      "[Note] Result <D: 1580009, L: 438604, V: 333005, C: 610461, T: 3243596, H: 2662967, E: 12772979, O: 28738, X: 2752, R: 83332, M: 24862>\n",
      "[Note] 162 number of value is NULL\n",
      "[DONE] Returned 21781467 items from ./ips/sk-jung-gu\n",
      "[Note] Setting Train/Test set separation.\n",
      "X_train.shape: (299576, 128, 6)\n",
      "y_train.shape: (299576, 6)\n",
      "[DONE] Train/Test set ready.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Testset Module: timesteps to learn and fold\n",
    "\"\"\"\n",
    "number_of_time_stamps = 128\n",
    "# Set general LSTM Data from keras\n",
    "data = get_data_list_on_folder(folder='./ips/sk-jung-gu', complete_set=False)\n",
    "(X_train, y_train), (X_test, y_test) = train_test_split(data, fold_size=0.1, time_steps=number_of_time_stamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6)\n",
      "(array([  850. ,   139. ,    25.4,  1023. ,    21.5,   200. ]), <type 'numpy.float64'>)\n",
      "(array([ 820. ,  294. ,   30.1,  693. ,   22.7,  100. ]), <type 'numpy.float64'>)\n",
      "[  671.51503124  4726.23268606    30.55859314   436.80005074    21.84991955\n",
      "   967.3454716 ]\n",
      "\n",
      "\n",
      "(128, 6)\n",
      "(array([ 447. ,    9. ,   30.2,  702. ,   22.2,  676. ]), <type 'numpy.float64'>)\n",
      "(array([ 390. ,   15. ,   30.2,  640. ,   22.8,  798. ]), <type 'numpy.float64'>)\n",
      "[ 369.27149403   30.83570481   37.48353129  288.6922103    20.99273182\n",
      "  578.14017846]\n"
     ]
    }
   ],
   "source": [
    "# D L V C T H\n",
    "print(np.shape(X_train[0])) # check value (timesteps, vectors)\n",
    "print(X_train[0][0], type(X_train[0][0][0])) # predicted shape\n",
    "print(y_train[0], type(y_train[0][0])) # predicted shape\n",
    "print(np.mean(y_train, axis=0))\n",
    "print(\"\\n\")\n",
    "print(np.shape(X_test[0]))\n",
    "print(X_test[0][0], type(X_test[0][0][0]))\n",
    "print(y_test[0], type(y_test[0][0]))\n",
    "print(np.mean(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 299576 samples, validate on 33172 samples\n",
      "Epoch 1/3\n",
      "299576/299576 [==============================] - 237s - loss: 565.1089 - val_loss: 267.1886\n",
      "Epoch 2/3\n",
      "299576/299576 [==============================] - 241s - loss: 513.9658 - val_loss: 265.7899\n",
      "Epoch 3/3\n",
      "299576/299576 [==============================] - 239s - loss: 500.8676 - val_loss: 264.9309\n",
      "[NOTE] Run on test set:\n",
      "33172/33172 [==============================] - 6s     \n",
      "GRU test accuracy: 78.00%\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    model.output_shape == (None, 32)\n",
    "    - Input shape   : 3D tensor (nb_samples, timesteps, input_dim)\n",
    "    - Output shape  : return_sequences 2D tensor is (nb_samples, output_dim)\n",
    "    - Note: https://keras.io/layers/recurrent/\n",
    "\"\"\"\n",
    "in_out_neurons = 6\n",
    "hidden_neurons = 128\n",
    "batch_size = 256\n",
    "# X_train.shape    # (# of samples, timesteps, output_dim)\n",
    "in_shape = (batch_size, number_of_time_stamps, in_out_neurons) # (batch_size, timesteps, input_dim)\n",
    "model_GRU = Sequential()\n",
    "model_GRU.add(GRU(output_dim=hidden_neurons,\n",
    "                    batch_input_shape=(batch_size, number_of_time_stamps, in_out_neurons),\n",
    "                    activation=\"sigmoid\",\n",
    "                    return_sequences=True))\n",
    "# model_GRU.add(Dropout(0.2))\n",
    "model_GRU.add(GRU(output_dim=hidden_neurons,\n",
    "                    batch_input_shape=(batch_size, number_of_time_stamps, hidden_neurons),\n",
    "                    activation=\"sigmoid\",\n",
    "                    return_sequences=False))\n",
    "# model_GRU.add(Dropout(0.2))\n",
    "# model_GRU.add(Dense(hidden_neurons, W_regularizer=l2(0.06)))\n",
    "model_GRU.add(Dense(output_dim=in_out_neurons, input_dim=hidden_neurons)) # None batches means any size of batch is able to process.\n",
    "model_GRU.add(Activation(\"softmax\")) # softmax relu sigmoid tanh linear.. https://keras.io/activations/\n",
    "\n",
    "model_GRU.compile(loss=\"poisson\", optimizer=\"adam\") \n",
    "# loss        : https://keras.io/objectives/#available-objectives \n",
    "# optimizers  : https://keras.io/optimizers/#usage-of-optimizers\n",
    "# metrics=['accuracy'], optimizer = \"sgd, rmsprop\"\n",
    "# Example: https://keras.io/getting-started/sequential-model-guide/\n",
    "model_GRU.fit(X_train, y_train, batch_size=batch_size, nb_epoch=3, validation_data=(X_test, y_test), verbose=1) # batch_size / validation_split=0.05\n",
    "# test model\n",
    "print(\"[NOTE] Run on test set:\")\n",
    "score = model_GRU.evaluate(X_test, y_test, batch_size=batch_size, show_accuracy=True, verbose=1)\n",
    "print(\"GRU test accuracy: %.2f%%\" % (score[1]*100))\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.740341006\n",
      "(33172, 6)\n",
      "(33172, 6)\n",
      "[ 0.18383579  0.00526818  0.01321167  0.35094127  0.00986537  0.43687773]\n",
      "[ 0.30764417  0.02458649  0.01860617  0.18876287  0.01273073  0.44766957]\n",
      "[ 382.    24.    34.9  912.    21.3  631. ]\n",
      "[ 369.27149403   30.83570481   37.48353129  288.6922103    20.99273182\n",
      "  578.14017846]\n"
     ]
    }
   ],
   "source": [
    "predicted = model_GRU.predict(X_test, batch_size=batch_size)\n",
    "print(np.sqrt((predicted - y_test) ** 2).mean(axis=0)).mean() # Printing RMSE\n",
    "# Check shape\n",
    "print(np.shape(predicted))\n",
    "print(np.shape(y_test))\n",
    "print(predicted[10])\n",
    "print(np.mean(predicted, axis=0))\n",
    "print(y_test[10])\n",
    "print(np.mean(y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Note] min error index is 17374. [ 0.43417633  0.03402258  0.02368315  0.0029541   0.01623598  0.48892787] vs [   8.    25.    39.     0.    21.7  333. ].\n",
      "[Note] min error index is 20264. [ 0.37753713  0.0281322   0.02494293  0.40161723  0.0163428   0.15142767] vs [   2.    35.    32.7  177.    22.7  171. ].\n",
      "[Note] min error index is 21596. [ 0.56357354  0.07097259  0.03120754  0.00359596  0.02399056  0.30665979] vs [ 242.    28.    33.     0.    19.5  128. ].\n",
      "[DONE] Check error done, min error is <425.71>. Max error is <3566.20>.\n"
     ]
    }
   ],
   "source": [
    "# check error minimum\n",
    "err_list = []\n",
    "for i in range(len(predicted)):\n",
    "    err = 0\n",
    "    for j in range(len(predicted[i])):\n",
    "        err += abs(predicted[i][j] - y_test[i][j])\n",
    "    if(err <= 450):\n",
    "        print('[Note] min error index is %d. %s vs %s.' % (i, str(predicted[i]), str(y_test[i])))\n",
    "    err_list.append(err)\n",
    "print('[DONE] Check error done, min error is <%.2f>. Max error is <%.2f>.' % (min(err_list), max(err_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Note] Calculating...\n",
      "[Note] D: 390.00, L: 15.00, V: 30.20, C: 640.00, T: 22.80, H: 798.00\n",
      "[ 390.    15.    30.2  640.    22.8  798. ]\n",
      "[ 0.22598445  0.00601583  0.01448026  0.37130883  0.01141506  0.37079549]\n"
     ]
    }
   ],
   "source": [
    "a = get_close_vector(vector=predicted[0], real_data=y_test, model_data=predicted)\n",
    "print(y_test[0])\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
